{"cells":[{"metadata":{"id":"QnRtnyuVrA03"},"cell_type":"markdown","source":"# New Plant Disease - Classification using Tensorflow 2.x"},{"metadata":{"id":"guYHIdDoNpmz","executionInfo":{"status":"ok","timestamp":1611106935579,"user_tz":-480,"elapsed":2120,"user":{"displayName":"Clémentine Pages","photoUrl":"https://lh4.googleusercontent.com/-aJemidvCafQ/AAAAAAAAAAI/AAAAAAAABHE/7k0YPWheeu4/s64/photo.jpg","userId":"02846512637567639137"}},"outputId":"a8629e00-4942-4171-ca1d-48ebcd060dcd","trusted":true},"cell_type":"code","source":"# Import TensorFlow into collab\nimport tensorflow as tf\nprint(f\"Tensorflow version: {tf.__version__}\")","execution_count":null,"outputs":[]},{"metadata":{"id":"RgrSWn1oAo5B","executionInfo":{"status":"ok","timestamp":1611108301050,"user_tz":-480,"elapsed":575,"user":{"displayName":"Clémentine Pages","photoUrl":"https://lh4.googleusercontent.com/-aJemidvCafQ/AAAAAAAAAAI/AAAAAAAABHE/7k0YPWheeu4/s64/photo.jpg","userId":"02846512637567639137"}},"outputId":"17a60457-b3cc-4250-abb4-831702ab28bb","trusted":true},"cell_type":"code","source":"import tensorflow as tf\ndevice_name = tf.test.gpu_device_name()\nif device_name != '/device:GPU:0':\n  raise SystemError('GPU device not found')\nprint('Found GPU at: {}'.format(device_name))","execution_count":null,"outputs":[]},{"metadata":{"id":"FhMZLRdnWdah","executionInfo":{"status":"ok","timestamp":1611108304245,"user_tz":-480,"elapsed":696,"user":{"displayName":"Clémentine Pages","photoUrl":"https://lh4.googleusercontent.com/-aJemidvCafQ/AAAAAAAAAAI/AAAAAAAABHE/7k0YPWheeu4/s64/photo.jpg","userId":"02846512637567639137"}},"trusted":true},"cell_type":"code","source":"# Import required packages\nimport os \nimport tensorflow as tf\nimport pandas as pd\nimport numpy as np\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"id":"0x7Yom0pXjMS","executionInfo":{"status":"ok","timestamp":1611108306901,"user_tz":-480,"elapsed":660,"user":{"displayName":"Clémentine Pages","photoUrl":"https://lh4.googleusercontent.com/-aJemidvCafQ/AAAAAAAAAAI/AAAAAAAABHE/7k0YPWheeu4/s64/photo.jpg","userId":"02846512637567639137"}},"outputId":"c53ffdfa-67d6-4cc8-8f88-e91cb7a84c5e","trusted":true},"cell_type":"code","source":"# Get Path to Train and Valid folders\ntrain_path = '../input/new-plant-diseases-dataset/New Plant Diseases Dataset(Augmented)/New Plant Diseases Dataset(Augmented)/train'\nvalid_path = '../input/new-plant-diseases-dataset/New Plant Diseases Dataset(Augmented)/New Plant Diseases Dataset(Augmented)/valid'\n\n# Get list of all subfolders for each Subset\ntrain_dir = os.listdir(train_path)\nvalid_dir = os.listdir(valid_path)\n# Check length of subfolders\nlen(train_dir), len(valid_dir)","execution_count":null,"outputs":[]},{"metadata":{"id":"hZNXOsXBWgoq","executionInfo":{"status":"ok","timestamp":1611108310765,"user_tz":-480,"elapsed":950,"user":{"displayName":"Clémentine Pages","photoUrl":"https://lh4.googleusercontent.com/-aJemidvCafQ/AAAAAAAAAAI/AAAAAAAABHE/7k0YPWheeu4/s64/photo.jpg","userId":"02846512637567639137"}},"outputId":"ce2e5667-a73a-409c-daa1-3b09bef75c90","trusted":true},"cell_type":"code","source":"# Create Dataframe\n\ndef create_info_df(path):\n    \"\"\"\n    input: `path` - folder path\n    From folder path, create a Dataframe with columns: \n    Plant | Category | Path | Plant___Category | Disease\n    return DataFrame\n    \"\"\"\n\n    list_plants = []\n    list_dir = os.listdir(path) # Get list direcotry\n    # Go through each folder to create url and get required information\n    for plant in list_dir:\n        url = path +'/'+plant\n        for img in os.listdir(url):\n            list_plants.append([*plant.split('___'), url+'/'+img, plant])\n\n    # Create DataFrame\n    df = pd.DataFrame(list_plants, columns=['Plant', 'Category', 'Path','Plant___Category'])\n    # Add `Disease` column - if folder name is not Healthy then plant is diseased\n    df['Disease'] = df.Category.apply(lambda x: 0 if x=='healthy' else 1)\n\n    return df\n\n# Get Validation and Training DF\ntrain_info = create_info_df(train_path)\nvalid_info = create_info_df(valid_path)\n\nprint(train_info.shape, valid_info.shape)\n\n#Unique label list:\nunique_plant_cat = np.unique(train_info['Plant___Category'].to_numpy())\nprint(\"Number of Categories to predict: \", len(unique_plant_cat))\n","execution_count":null,"outputs":[]},{"metadata":{"id":"AR4dWqeCXSjL","executionInfo":{"status":"ok","timestamp":1611108321057,"user_tz":-480,"elapsed":625,"user":{"displayName":"Clémentine Pages","photoUrl":"https://lh4.googleusercontent.com/-aJemidvCafQ/AAAAAAAAAAI/AAAAAAAABHE/7k0YPWheeu4/s64/photo.jpg","userId":"02846512637567639137"}},"trusted":true},"cell_type":"code","source":"# Creation of constants\nIMG_SIZE = 64\nIMG_SHAPE = (IMG_SIZE, IMG_SIZE)\nbatch_size = 32\nAUTOTUNE = tf.data.experimental.AUTOTUNE\nOUTPUT_SHAPE = 38\nNUM_EPOCHS = 10","execution_count":null,"outputs":[]},{"metadata":{"id":"GteG5CMwWsgn","executionInfo":{"status":"ok","timestamp":1611108326655,"user_tz":-480,"elapsed":912,"user":{"displayName":"Clémentine Pages","photoUrl":"https://lh4.googleusercontent.com/-aJemidvCafQ/AAAAAAAAAAI/AAAAAAAABHE/7k0YPWheeu4/s64/photo.jpg","userId":"02846512637567639137"}},"trusted":true},"cell_type":"code","source":"## FUNCTION UTILS - Prepare Data and Dataset ##\n\ndef create_img_df(df_info, frac=0.1, random_state=42):\n    return df_info.sample(frac=frac, random_state=random_state).reset_index()\n\ndef create_train_val_df(valid_info, train_info, frac=0.1, random_state=42):\n    \"\"\"\n    Create Train and validation dataframe\n    Return:\n      - train dataframe\n      - validation dataframe\n    \"\"\"\n    valid_df = create_img_df(valid_info, frac, random_state)\n    train_df = create_img_df(train_info, frac, random_state)\n    \n    # Get information shape\n    valid_img_cnt,train_img_count = valid_df.shape[0], train_df.shape[0]\n    total = valid_img_cnt + train_img_count\n    # Print information\n    print(f'Total images (frac={frac}): ', total)\n    print(f\"Training ({train_img_count}): {train_img_count/total*100:.2f}% - Validation ({valid_img_cnt}): {valid_img_cnt/total*100:.2f}%\")\n     \n    return train_df, valid_df\n\n\ndef get_bool_label(labels):\n    # Create a variable of all Labels\n    plant_cat_labels = labels.to_numpy()\n    # Create Boolean label list\n    bool_plant_cat = [unique_plant_cat == plant_cat for plant_cat in plant_cat_labels]\n    # return array\n    return bool_plant_cat\n    \n# Prepare Data\ndef prepare_data(train_df, valid_df):\n    \"\"\"\n    Get Train and Validation Data Frame and return X_train, X_val, y_train, y_val\n    \"\"\"\n    # create images (X) arrays\n    X_train = train_df['Path']\n    X_val = valid_df['Path']\n    \n    # create labels (y) arrays\n    y_train = get_bool_label(train_df['Plant___Category'])\n    y_val = get_bool_label(valid_df['Plant___Category'])\n    \n    print('Shape: ',X_train.shape, X_val.shape, len(y_train), len(y_val))\n    \n    return X_train, X_val, y_train, y_val\n    \n\n\n# Dataset function utils\n\n# Decode and load image\ndef decode_img(path, img_shape=IMG_SHAPE):\n    \"\"\"\n    Read image from `path`, and convert the image to a 3D tensor\n    return resized image.\n    input: `path`: Path to an image\n    return: resized tensor image\n    \"\"\"\n    print('Image size: ({})'.format(img_shape))\n    # Read the image file\n    img = tf.io.read_file(path)\n    img = tf.image.decode_jpeg(img, channels=3)\n    img = tf.cast(img, tf.float32)/255\n    # Resize image to our desired size\n    img = tf.image.resize(img, img_shape)\n    return img\n\n# Configure dataset for performance\ndef configure_for_performance(ds):\n    #ds = ds.cache()\n    ds = ds.batch(batch_size)\n    #ds = ds.prefetch(buffer_size=AUTOTUNE)\n    return ds\n\n# Create a function to get Dataset\ndef create_dataset(X, y=None, valid_data=False, test_data=False, img_shape=IMG_SHAPE):\n    \"\"\"\n    Create Dataset from Images (X) and Labels (y)\n    Shuffles the data if it's training data but doesn't shuffle if it validation data.\n    Also accepts test data as input (no labels).\n    Return Dataset \n    \"\"\"\n    print(\"Creating data set...\")\n    # If test data, there is no labels\n    if test_data:       \n        print(\"Creating test data batches...\")\n        dataset = tf.data.Dataset.from_tensor_slices((X))\n        dataset = dataset.map(lambda x: decode_img(x, img_shape), num_parallel_calls=AUTOTUNE)\n        dataset = configure_for_performance(dataset)\n    # If Valid_data - we don't need to shuffle\n    elif valid_data:\n        print(\"Creating Valid data batches...\")\n        dataset = tf.data.Dataset.from_tensor_slices((X, y))\n        dataset = dataset.map(lambda x, y: [decode_img(x, img_shape), y], num_parallel_calls=AUTOTUNE)\n        dataset = configure_for_performance(dataset)\n    else:\n        print(\"Creating Training data batches...\")\n        dataset = tf.data.Dataset.from_tensor_slices((X, y))\n        dataset = dataset.map(lambda x, y: [decode_img(x, img_shape), y], num_parallel_calls=AUTOTUNE)\n        dataset = dataset.shuffle(buffer_size=len(X))\n        dataset = configure_for_performance(dataset)\n           \n    print(dataset.element_spec)\n\n    return dataset\n\n\n# Create Models function utils #\n################################\n\n# Callbacks\n# Early stopping Callbacks\nearly_stopping = tf.keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", patience=5)\n\n# Reduce Learning rate Callbacks\nlr_callback = tf.keras.callbacks.ReduceLROnPlateau(monitor = 'val_loss',\n                                                   patience=3,\n                                                   factor=0.2,\n                                                   verbose=2,\n                                                   mode='min')\n\n\n# Useful Functions for Model training, saving and loading\n\ndef train_model(transfer_model, epochs = NUM_EPOCHS):\n    \"\"\"  \n    Trains a given model and returns the trained version.\n     Input: model, number of Epochs (default = NUM_EPOCHS)\n     Output: model\n    \"\"\"\n    # create model\n    model = create_model(transfer_model)\n    # Create TensorBoard session\n    tensorboard = create_tensorboard_callback()\n\n    model.summary()\n    print(f\"Information: epochs = {epochs} and number of images = {NUM_IMAGES}\")\n\n    # Fit model\n    model.fit(x=dataset_train,\n              epochs=epochs,\n              validation_data=dataset_val,\n              callbacks=[early_stopping, lr_callback])\n    return model\n\nimport datetime\n# Save and load model\n# Create a function to save a model\ndef save_model(model, suffix=None):\n    \"\"\"\n    Saves a given model in ad models directory and appends a suffix (string).\n    \"\"\"\n    # Create a model directory pathname with current time\n    modeldir = os.path.join(\"../output/kaggle/working/saved_models\",\n                          datetime.datetime.now().strftime(\"%Y%m%d-%H%M%s\"))\n    model_path = modeldir + \"-\" + suffix + \".h5\" #save format of model\n    print(f\"Save model to: {model_path}...\")\n    model.save(model_path)\n    return model_path\n\n# Create a function to load a model\ndef load_model(model_path):\n    \"\"\"\n    Load a saved model from a specify path\n    \"\"\"\n    print(f\"Loading saved model from: {model_path}...\")\n    model = tf.keras.models.load_model(model_path)\n    return model","execution_count":null,"outputs":[]},{"metadata":{"id":"-x_3dkqebCf0","executionInfo":{"status":"ok","timestamp":1611108342110,"user_tz":-480,"elapsed":2579,"user":{"displayName":"Clémentine Pages","photoUrl":"https://lh4.googleusercontent.com/-aJemidvCafQ/AAAAAAAAAAI/AAAAAAAABHE/7k0YPWheeu4/s64/photo.jpg","userId":"02846512637567639137"}},"outputId":"43192b81-45e9-46c6-aa46-7678ae35ee94","trusted":true},"cell_type":"code","source":"\n#Create and get dataset\nFRAC = 1\nIMG_SIZE = 64\nIMG_SHAPE = (IMG_SIZE, IMG_SIZE)\nOUTPUT_SHAPE = len(unique_plant_cat)\n\ntrain_df, valid_df = create_train_val_df(valid_info, train_info, frac=FRAC)\n# Get data ready\nX_train, X_val, y_train, y_val = prepare_data(train_df, valid_df)\n\n# Create Dataset #\n##################\n# Train dataset - shuffle \ndataset_train = create_dataset(X_train, y_train, img_shape=IMG_SHAPE)\n# Validation Dataset - not shuffle\ndataset_val = create_dataset(X_val, y_val, valid_data=True, img_shape=IMG_SHAPE)\n# Verify length of both datasets\nlen(dataset_train), len(dataset_val)\n\nNUM_IMAGES = len(y_train) + len(y_val)","execution_count":null,"outputs":[]},{"metadata":{"id":"dE28XuKBbaHR","executionInfo":{"status":"ok","timestamp":1611108646861,"user_tz":-480,"elapsed":629,"user":{"displayName":"Clémentine Pages","photoUrl":"https://lh4.googleusercontent.com/-aJemidvCafQ/AAAAAAAAAAI/AAAAAAAABHE/7k0YPWheeu4/s64/photo.jpg","userId":"02846512637567639137"}},"outputId":"436e87e2-8390-4c04-e4bc-27a50681986c","trusted":true},"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, Conv2D, MaxPooling2D, Flatten, BatchNormalization, Activation\nIMG_SIZE=64\nNUM_EPOCHS = 10\nINPUT_SHAPE = (IMG_SIZE, IMG_SIZE, 3)\n\n# Create model\n# 4 conv2D layers\n# Batch Normalisation and MaxPooling\ndef get_model():\n    \"\"\"\n    Create a 4 Conv2D layers with\n    - Batch Normalisation\n    - MaxPooling\n    - ReLU activation\n    And 2 Dense layers reLU activation (and Dropout)\n\n    Return 38 probabilities (= number of plants we want to predict) - activation Softmax\n    \"\"\"\n\n    model_v2 = Sequential([\n        # First CNN                     \n        Conv2D(128, kernel_size=3, input_shape=INPUT_SHAPE, activation='relu'),\n        MaxPooling2D(),\n        BatchNormalization(),\n        # Second CNN\n        Conv2D(256, kernel_size=3, activation='relu'),   \n        MaxPooling2D(),\n        BatchNormalization(),\n        # Third CNN\n        Conv2D(512, kernel_size=3, activation='relu'), \n        MaxPooling2D(),\n        BatchNormalization(),\n        # Flatten last CNN output for Dense layers\n        Flatten(),\n        Dense(512, activation='relu'),\n        Dropout(0.2),\n        Dense(256, activation='relu'),\n        Dropout(0.2),\n        # Return 38 probabilities (= number of plants we want to predict)\n        Dense(OUTPUT_SHAPE, activation= 'softmax')\n    ])\n\n    return model_v2\n\n# To Do: modify Adam optimizer and add a specific Learning rate\nmodel = get_model()\nmodel.compile(optimizer=tf.optimizers.Adam(learning_rate=0.0005),\n            loss='categorical_crossentropy',\n            metrics=['accuracy']\n            )\n# Show Summary \nmodel.summary()\n","execution_count":null,"outputs":[]},{"metadata":{"id":"4MxaB8mB5en8","executionInfo":{"status":"ok","timestamp":1611110742850,"user_tz":-480,"elapsed":2078477,"user":{"displayName":"Clémentine Pages","photoUrl":"https://lh4.googleusercontent.com/-aJemidvCafQ/AAAAAAAAAAI/AAAAAAAABHE/7k0YPWheeu4/s64/photo.jpg","userId":"02846512637567639137"}},"outputId":"f464f2c1-1ce8-4545-bf26-fab5f2b13f85","trusted":true},"cell_type":"code","source":"# Train Model\nhistory = model.fit(x=dataset_train,\n                  epochs=NUM_EPOCHS,\n                  validation_data=dataset_val,\n                  callbacks=[early_stopping, lr_callback])\n\n# Get Validation Loss and Accuracy\nval_loss, val_acc = model.evaluate(dataset_val)\nval_acc = round(val_acc, 3)\n\n# Save model\nsuffix = 'tf_ep-'+str(NUM_EPOCHS)+'_img-'+str(NUM_IMAGES)+'_acc_'+str(val_acc)+'-model_cnn_'+str(FRAC)\nsave_model(model, suffix=suffix)\n","execution_count":null,"outputs":[]},{"metadata":{"id":"xGuNa6WSkNEh","executionInfo":{"status":"ok","timestamp":1611110920328,"user_tz":-480,"elapsed":714,"user":{"displayName":"Clémentine Pages","photoUrl":"https://lh4.googleusercontent.com/-aJemidvCafQ/AAAAAAAAAAI/AAAAAAAABHE/7k0YPWheeu4/s64/photo.jpg","userId":"02846512637567639137"}},"trusted":true},"cell_type":"code","source":"# convert the history.history dict to a pandas DataFrame:     \nhist_df = pd.DataFrame(history.history) \n\ndate = datetime.datetime.now().strftime(\"%Y%m%d\")\nhist_csv_file = '../output/kaggle/working/history_full_'+str(val_acc)+'_'+str(date)+'.csv'\nwith open(hist_csv_file, mode='w') as f:\n    hist_df.to_csv(f)","execution_count":null,"outputs":[]},{"metadata":{"id":"oEkmpu7FrtYa","trusted":true},"cell_type":"code","source":"# Useful functions to evaluate model\n\n# Turn prediction probabilities into their respective label (easier to understand)\ndef get_pred_label(prediction_probabilities):\n    \"\"\"\n    Turns an array of prediction probabilities into a label.\n    \"\"\"\n    return unique_plant_cat[np.argmax(prediction_probabilities)]\n\n# Create a function to unbatch a batch dataset\ndef unbatchify(batch_data):\n    \"\"\"\n    Take batch data and return unbatch data (separate arrays of images and labels) in a form of a tuple of lists\n    \"\"\"\n    img = []\n    lbl = []\n    for image, label in batch_data.unbatch().as_numpy_iterator():\n        img.append(image*255)\n        lbl.append(get_pred_label(label))\n\n    return img,lbl\n\n# Show images and prediction rate\ndef show_img_and_prediction(model, nb_img=9):\n    # Get predictions\n    predictions = model.predict(dataset_val)\n    # Get Validation datset images and true labels\n    imgs, labels = unbatchify(dataset_val)\n    # Get 10 random images in the validation dataset\n    img_rdm = np.random.randint(0, len(imgs), nb_img)\n\n    plt.figure(figsize=(20,12))\n    for idx, i in enumerate(img_rdm):\n        color = 'red'\n\n        plt.subplot(3,3,idx+1)\n        plt.imshow(imgs[i].astype('uint8'))\n        plt.xticks([])\n        plt.yticks([])\n\n        if get_pred_label(predictions[i]) == labels[i]:\n            color = 'green'\n\n        plt.title('Pred({}) : {} - {:2.0f}%'.format(i, get_pred_label(predictions[i]), np.max(predictions[i])*100), color=color)\n        plt.xlabel('Real: {}'.format(labels[i]));    \n\n\ndef plot_acc_and_loss(history):\n    \"\"\"\n    From Model History, plot two Graphs: \n    - Accuracy Train + Validation\n    - Loss Train + Validation\n\n    Input: model history\n    \"\"\"\n    acc = history.history['accuracy']\n    val_acc = history.history['val_accuracy']\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n    epochs = range(1, len(loss)+1)\n\n    plt.figure(figsize=(16,10))\n\n    plt.subplot(121)\n    plt.plot(epochs, acc, color='red', label='Training Accuracy')\n    plt.plot(epochs, val_acc, color='blue', label='Validation Accuracy')\n    plt.title('Training and Validation Accuracy')\n    plt.ylabel('Accuracy')\n    plt.xlabel('Epoch')\n    plt.xticks(epochs)\n    plt.yticks(np.arange(0,1.1,0.1))\n    plt.legend()\n\n    plt.subplot(122)\n    plt.plot(epochs, loss, color='orange', label='Training Loss')\n    plt.plot(epochs, val_loss, color='navy', label='Validation Loss')\n    plt.title('Training and Validation Loss')\n    plt.ylabel('Loss')\n    plt.xlabel('Epoch')\n    plt.xticks(epochs)\n    plt.legend()\n\n\ndef plot_pred_prob(predictions, labels, n=1):\n    \"\"\"\n    Show the top 3 highest prediction confidences along with the truth label for sample n.\n    \n    Inputs: \n      - predictions array \n      - labels array\n      - n id of sample to check \n    \"\"\"\n    pred_prob, true_label = predictions[n], labels[n]\n\n    # Get predicted label\n    pred_label = get_pred_label(pred_prob)\n\n    # Get top 3 prediction confidence indexes\n    top_3_pred_indexes = pred_prob.argsort()[-3:][::-1]\n    # Find the top 3 prediction confidence values\n    top_3_pred_values = pred_prob[top_3_pred_indexes]\n    # Find the top 3 prediction labels\n    top_3_pred_labels = unique_plant_cat[top_3_pred_indexes]\n\n    # Setup plot\n    \n    top_plot = plt.barh(np.arange(len(top_3_pred_labels)),\n                     top_3_pred_values,\n                     color=\"grey\")\n    plt.yticks(np.arange(len(top_3_pred_labels)),\n             labels=top_3_pred_labels,\n             rotation=\"horizontal\")\n    plt.xlabel('Probability')\n  \n    # Change color of true label\n    if np.isin(true_label, top_3_pred_labels):\n        top_plot[np.argmax(top_3_pred_labels == true_label)].set_color(\"green\")\n        if top_3_pred_labels[0] != true_label:\n            top_plot[0].set_color(\"red\")\n        else:\n            pass\n\ndef get_wrong_preds(predictions, labels, n=9):\n\n    pred_idx = []\n\n    for i, pred_prob in enumerate(predictions):\n        pred_label = get_pred_label(pred_prob)\n        if pred_label != labels[i]:\n            pred_idx.append(i)\n\n        if len(pred_idx) >= n:\n            return pred_idx\n\n    return pred_idx","execution_count":null,"outputs":[]},{"metadata":{"id":"Y6g_vIsCpgIF","trusted":true},"cell_type":"code","source":"# Get predictions\npredictions = model.predict(dataset_val)\n# Get Validation datset images and true labels\nimgs, labels = unbatchify(dataset_val)\n","execution_count":null,"outputs":[]},{"metadata":{"id":"fYZdovYLEk-L","executionInfo":{"status":"ok","timestamp":1610421332840,"user_tz":-480,"elapsed":1320,"user":{"displayName":"Clémentine Pages","photoUrl":"https://lh4.googleusercontent.com/-aJemidvCafQ/AAAAAAAAAAI/AAAAAAAABHE/7k0YPWheeu4/s64/photo.jpg","userId":"02846512637567639137"}},"outputId":"c634ea92-86d4-4253-d527-9c22236c537f","trusted":true},"cell_type":"code","source":"# Plot Accuracy & Loss\nplot_acc_and_loss(history)","execution_count":null,"outputs":[]},{"metadata":{"id":"TMc5R1zmku1z","executionInfo":{"status":"ok","timestamp":1610421940477,"user_tz":-480,"elapsed":28758,"user":{"displayName":"Clémentine Pages","photoUrl":"https://lh4.googleusercontent.com/-aJemidvCafQ/AAAAAAAAAAI/AAAAAAAABHE/7k0YPWheeu4/s64/photo.jpg","userId":"02846512637567639137"}},"outputId":"a3a4b552-045b-48f2-8bd1-392387a84c6b","trusted":true},"cell_type":"code","source":"show_img_and_prediction(model)","execution_count":null,"outputs":[]},{"metadata":{"id":"MgWPA6M3nKWD","executionInfo":{"status":"ok","timestamp":1610424805890,"user_tz":-480,"elapsed":1590,"user":{"displayName":"Clémentine Pages","photoUrl":"https://lh4.googleusercontent.com/-aJemidvCafQ/AAAAAAAAAAI/AAAAAAAABHE/7k0YPWheeu4/s64/photo.jpg","userId":"02846512637567639137"}},"outputId":"aa27dda1-4d45-4411-f12a-3111bde9d697","trusted":true},"cell_type":"code","source":"#wrong_pred_idx\nwrong_pred_idx = get_wrong_preds(predictions, labels, n=6)\n\nplt.figure(figsize=(20,18))\nplt.subplots_adjust(wspace = 0.6)\nfor i, pred_idx in enumerate(wrong_pred_idx):\n    plt.subplot(3,3,i+1)\n    plot_pred_prob(predictions, labels, n=pred_idx)\n    plt.title('Real: {}'.format(labels[pred_idx]))\n","execution_count":null,"outputs":[]},{"metadata":{"id":"IrLgJjobolf8","executionInfo":{"status":"ok","timestamp":1610936854404,"user_tz":-480,"elapsed":657,"user":{"displayName":"Clémentine Pages","photoUrl":"https://lh4.googleusercontent.com/-aJemidvCafQ/AAAAAAAAAAI/AAAAAAAABHE/7k0YPWheeu4/s64/photo.jpg","userId":"02846512637567639137"}},"outputId":"5276267d-2c0c-416a-ac60-596c0c1d13de","trusted":true},"cell_type":"code","source":"# Get test data\nimport os\nimport pandas as pd\n\n\ntest_path = '../input/new-plant-diseases-dataset/test/test'\n\ntest_imgs = [os.path.join(test_path,img) for img in os.listdir(test_path)]\ndf_test = pd.DataFrame(test_imgs, columns=['Path'])\n\ndf_test.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"SKYJIYv9HhkT","executionInfo":{"status":"ok","timestamp":1610590903659,"user_tz":-480,"elapsed":727,"user":{"displayName":"Clémentine Pages","photoUrl":"https://lh4.googleusercontent.com/-aJemidvCafQ/AAAAAAAAAAI/AAAAAAAABHE/7k0YPWheeu4/s64/photo.jpg","userId":"02846512637567639137"}},"outputId":"30e88b43-962f-487c-9d9a-ca0bff2b676d","trusted":true},"cell_type":"code","source":"#Create Test Dataset\nIMG_SIZE = 64\nIMG_SHAPE = (IMG_SIZE, IMG_SIZE)\n\ntest_dataset = create_dataset(df_test['Path'], test_data=True, img_shape=IMG_SHAPE)","execution_count":null,"outputs":[]},{"metadata":{"id":"4FQfhKA8uk2q","trusted":true},"cell_type":"code","source":"# Create a DF with Predictions\npreds_df = pd.DataFrame(columns=[\"id\"] + list(unique_plant_cat))\n# Append test image ID's to prediction DataFrame\ntest_ids = [os.path.splitext(path)[0] for path in os.listdir(test_path)]\npreds_df[\"id\"] = test_ids","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_preds = model.predict(test_dataset)","execution_count":null,"outputs":[]},{"metadata":{"id":"tEaUdriRwBbN","executionInfo":{"status":"ok","timestamp":1610593444526,"user_tz":-480,"elapsed":949,"user":{"displayName":"Clémentine Pages","photoUrl":"https://lh4.googleusercontent.com/-aJemidvCafQ/AAAAAAAAAAI/AAAAAAAABHE/7k0YPWheeu4/s64/photo.jpg","userId":"02846512637567639137"}},"outputId":"ee60ec1b-dea0-4647-e6a2-cde008620836","trusted":true},"cell_type":"code","source":"# Add the prediction probabilities to each plants category columns\npreds_df[list(unique_plant_cat)] = test_preds\npreds_df.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"KBNim4dctac6","executionInfo":{"status":"ok","timestamp":1610593655781,"user_tz":-480,"elapsed":5149,"user":{"displayName":"Clémentine Pages","photoUrl":"https://lh4.googleusercontent.com/-aJemidvCafQ/AAAAAAAAAAI/AAAAAAAABHE/7k0YPWheeu4/s64/photo.jpg","userId":"02846512637567639137"}},"outputId":"a979775f-cf3c-4435-d5c5-a39d86605223","trusted":true},"cell_type":"code","source":"# Show Test Images\nimport matplotlib.image as mpimg\n\nimages_test = []\nfor img_path in df_test['Path']:\n    images_test.append(mpimg.imread(img_path))\n\nplt.figure(figsize=(20,40))\nfor i, image in enumerate(images_test):\n    plt.subplot(11,3,i+1)\n    plt.imshow(image)\n    plt.title('Pred: {} - {:2.0f}%'.format( get_pred_label(test_preds[i]), np.max(test_preds[i])*100))\n    plt.xlabel(f'Real: {test_ids[i]}', fontsize=13, color='blue')\n    plt.xticks([])\n    plt.yticks([])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert the model.\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\ntflite_model = converter.convert()\n\n# Save the model.\nwith open('model.tflite', 'wb') as f:\n    f.write(tflite_model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save(\"modelplant.h5\")","execution_count":null,"outputs":[]},{"metadata":{"id":"v5HYtJ_l6EUz","trusted":true},"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\n\n# Load TFLite model and allocate tensors.\ninterpreter = tf.lite.Interpreter(model_path=\"../input/plant-model/model.tflite\")\ninterpreter.allocate_tensors()\n\n# Get input and output tensors.\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Test model on random input data.\ninput_shape = input_details[0]['shape']\ninput_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)\ninterpreter.set_tensor(input_details[0]['index'], input_data)\n\ninterpreter.invoke()\n\n# The function `get_tensor()` returns a copy of the tensor data.\n# Use `tensor()` in order to get a pointer to the tensor.\noutput_data = interpreter.get_tensor(output_details[0]['index'])\nprint(output_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset_train.class_indices","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}